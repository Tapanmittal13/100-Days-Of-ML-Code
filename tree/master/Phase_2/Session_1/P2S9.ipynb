{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2S9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tapanmittal13/100-Days-Of-ML-Code/blob/master/tree/master/Phase_2/Session_1/P2S9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-aGHQYB_Fv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7HJ2veh_VaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object): \n",
        "\t\n",
        "\tdef __init__(self, max_size = 1e6): \n",
        "\t\tself.storage = [] \n",
        "\t\tself.max_size = max_size \n",
        "\t\tself.ptr = 0\n",
        "\n",
        "\n",
        "\tdef add(self, transition): \n",
        "\t\tif len(self.storage) == self.max_size: \n",
        "\t\t\tself.storage[int(self.ptr)] = transition \n",
        "\t\t\tself.ptr = (self.ptr + 1) % self.max_size \n",
        "\t\telse: \n",
        "\t\t\tself.storage.append(transition) \n",
        "\n",
        "\n",
        "\tdef sample(self, batch_size): \n",
        "\t\tind = np.random.randint(0, len(self.storage), batch_size) \n",
        "\t\tbatch_states, batch_next_states, batch_actions, batch_rewards,batch_dones = [], [], [], [], [] \n",
        "\t\tfor i in ind: \n",
        "\t\t\tstate, next_state, action, reward, done = self.storage[i] \n",
        "\t\t\tbatch_states.append(np.array(state, copy = False)) \n",
        "\t\t\tbatch_next_states.append(np.array(next_state, copy = False)) \n",
        "\t\t\tbatch_actions.append(np.array(action, copy = False)) \n",
        "\t\t\tbatch_rewards.append(np.array(reward, copy = False)) \n",
        "\t\t\tbatch_dones.append(np.array(done, copy = False)) \n",
        "\t\treturn np.array(batch_states), np.array(batch_next_states), \\\n",
        "\t\t\tnp.array(batch_actions), np.array(batch_rewards).reshape(-1, 1),\\\n",
        "\t\t\tnp.array(batch_dones).reshape(-1, 1) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V76_udSlCnTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "\tdef __init__(self, state_dims, action_dim, max_action): \n",
        "\t\t#fmax action is to clip in case we added too much noise \n",
        "\t\tsuper(Actor, self).__init__() #activate the inheritance \n",
        "\t\tself.layer_1 = nn.Linear(state_dims, 400) \n",
        "\t\tself.layer_2 = nn.Linear(400, 300) \n",
        "\t\tself.layer_3 = nn.Linear(300, action_dim) \n",
        "\t\tself.max_action = max_action\n",
        "\n",
        "\tdef forward(self, x): \n",
        "\t\tx = F.relu(self.layer_1(x)) \n",
        "\t\tx = F.relu(self.layer_2(x)) \n",
        "\t\tx = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "\t\treturn x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHTHjmiSCrEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Critic(nn.Module): \n",
        "\n",
        "\tdef __init__(self, state_dims, action_dim): \n",
        "\t\t#max action is to clip in case we added too much noise \n",
        "\t\tsuper(Critic, self).__init__() # activate the inheritance \n",
        "\t\t# First Critic Network \n",
        "\t\tself.layer_1 = nn.Linear(state_dims + action_dim, 400) \n",
        "\t\tself.layer_2 = nn.Linear(400, 300) \n",
        "\t\tself.layer_3 = nn.Linear(300, action_dim) \n",
        "\n",
        "\t\t# Second Critic Network \n",
        "\t\tself.layer_4 = nn.Linear(state_dims + action_dim, 400) \n",
        "\t\tself.layer_5 = nn. Linear(400, 300) \n",
        "\t\tself.layer_6 = nn.Linear(300, action_dim)\n",
        "\n",
        "\n",
        "\tdef forward(self, x, u): # x - state, u = action \n",
        "\t\txu = torch.cat([x, u], 1) # 1 for verticat concatenation, 0 for Hzntal \n",
        "\t\t# forward propagation on first Critic \n",
        "\t\tx1 = F.relu(self.layer_1(xu)) \n",
        "\t\tx1 = F.relu(self.layer_2(x1)) \n",
        "\t\tx1 = self.layer_3(x1) \n",
        "\t\t# forward propagation on second Critic \n",
        "\t\tx2 = F.relu(self.layer_4(xu)) \n",
        "\t\tx2 = F.relu(self.layer_5(x2)) \n",
        "\t\tx2 = self.layer_6(x2) \n",
        "\t\treturn x1, x2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlJTYpfTEwbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Selecting the device (CPU or UPU) \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "\n",
        "\n",
        "# Building the whole Training Proces into a class \n",
        "class T3D(object): \n",
        "\n",
        "\tdef __init__(self, state_dims, action_dim, max_action):\n",
        "\t\t# making sure our T3D class can work with any env \n",
        "\t\tself.actor = Actor(state_dims, action_dim, max_action).to(device) # GD \n",
        "\t\tself.actor_target = Actor(state_dims, actiondim, max_action).to(device) # Potyok Avg \n",
        "\t\tself.actor_target.load_statedict(self.actor.state_dict) # initializing with model weights to keep the same \n",
        "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "\n",
        "\t\tself.critic = Critic(state_dims, action_dim).to(device) # GD \n",
        "\t\tself.critic_target = critic(state_dims, action_dim).to(device) # Potyak Avg \n",
        "\t\tself.critic_target.load_state_dict(self.critic.state_dict)\n",
        "\n",
        "\t\t# initializing with model weights to keep them same \n",
        "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters()) \n",
        "\t\tself.maxaction = maxaction \n",
        "\n",
        "\n",
        "\tdef select_action(self, state): \n",
        "\t\tstate = torch.Tensor(state.reshape(1, -1)).to(device) \n",
        "\t\treturn self.actor(state).cpu().data.numpy().flatten() \n",
        "\t\t# need to convert to numpy, remember clipping? \n",
        "\n",
        "\n",
        "\tdef train(self, replay_buffer, iterations, batchsize=100, discount=0.99,  tau = 0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "\t\tfor it in range(iterations): \n",
        "\t\t\t# Step 4 We sample from a batch of transitions (s, s', a, r) from memory \n",
        "\t\t\tbatch_states, batch_next_states, batch_actions, batch_rewards, batchdones = replay_buffer.sample(batch_size) \n",
        "\t\t\tstate = torch.Tensor(batchstates).to(device) \n",
        "\t\t\tnext_state = torch.Tensor(batchnextstates).to(device) \n",
        "\t\t\taction = torch.Tensor(batch_actions).to(device) \n",
        "\t\t\treward = torch.Tensor(batch_rewards).to(device) \n",
        "\t\t\tdone = torch.Tensor(batch_dones).to(device) \n",
        "\n",
        "\t\t\t# Step 5: From the next state the Actor target plays the next actions a' \n",
        "\t\t\tnext_action = self.actor_target.forward(next_state)\n",
        "\n",
        "\t\t\t# Step 6, We add Gaussian noise to this next action a and we clamp it in a \n",
        "\t\t\t# range of values supported by the environment \n",
        "\t\t\tnoise = torch.Tensor(batch_actions).data.normal(0, policy_noise).to(device) \n",
        "\t\t\tnoise = noise.clamp(-noise_clip, noise_clip) \n",
        "\t\t\tnext_action = (next_action + noise).clamp(-self.max_action, self.max_action) \n",
        "\n",
        "\t\t\t#Step 7: The two critic targets take each the couple (s', a') as input and\n",
        "\t\t\t# return two Q-values , Qt1(s',a') and Qt2(s',a') as output\n",
        "\t\t\ttarget_Q1, target_Q2 = self.critic_target.forward(next_state, next_action)\n",
        "\n",
        "\n",
        "\t\t\t# Step 8: We keep the minimum of these two Q-Values \n",
        "\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "\t\t\t# Step 9: We get the final target of the two Critic model, which is:\n",
        "\t\t\t# Qt = r + gamma*min(Qt1, Qt2)\n",
        "\n",
        "\t\t\t# target_Q = reward + (1 - done) * discount * target_Q\n",
        "\n",
        "\t\t\t# 0 = episode not over, 1 = episode over\n",
        "\n",
        "\t\t\t# We canâ€™t run the above equation efficiently as some components are in Computational\n",
        "\t\t\t# graphs and some are not, We need to make minor modification\n",
        "\n",
        "\t\t\ttarget_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "\n",
        "\t\t\t# Step 10 : Te two critic models take each the couple (s,a),\n",
        "\t\t\t# as input and return two Q Values\n",
        "\t\t\tcurrent_Ql, current_Q2 = self.critic.forward(state, action)\n",
        "\n",
        "\n",
        "\t\t\t# Step 11: We compute the Loss coming from the two Critic models \n",
        "\t\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q) \n",
        "\n",
        "\n",
        "\t\t\t# Step 12, We backpropagate this Critic Loss snd update the parameters of the two Critic \n",
        "\t\t\t# models with a Adam optimizer \n",
        "\t\t\tself.critic_optimizer.zero_grad() #initiatizing the gradients to zero \n",
        "\t\t\tcritic_loss.backward() # computing the gradients \n",
        "\t\t\tself.critic_optimizer.step() #performing the weight updates \n",
        "\n",
        "\n",
        "\t\t\t# Step 13: Once every two iterations, we update our Actor cadet by \n",
        "\t\t\t# performing gradient asent on the output of the first critic model \n",
        "\t\t\tif it % policy_freq == 0: \n",
        "\t\t\t\t# This is OPG part \n",
        "\t\t\t\tactor_loss = -(self.critic.Q1(state, self.actor(state)).mean()) \n",
        "\t\t\t\tself.actor_optimizer.grad_zero() \n",
        "\t\t\t\tactor_loss.backward() \n",
        "\t\t\t\tself.actor_optimizer.step() \n",
        "\n",
        "\n",
        "\t\t\t\t# Step 14: Still once every two iteractions, we update the weights of the Actor target \n",
        "\t\t\t\t# by PoLyak averaging \n",
        "\t\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()): \n",
        "\t\t\t\t\ttarget_param.data.copy_(tau * param.data + (1 - tau) * target_param.data) \n",
        "\n",
        "\n",
        "\t\t\t\t# Step 15: Stift once every two iterations, we update the weights of the Critic target gi \n",
        "\t\t\t\t# by Pcqyak averang \n",
        "\t\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()): \n",
        "\t\t\t\t\ttarget_param.data.copy_(tan * param.data + (1 - tau) * target_param.data) \n",
        "\t\t\t\t\n",
        "\t\t\t\t# T3D is done now! \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1qD0ks3IPIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}